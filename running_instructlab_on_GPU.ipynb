{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7ih7e5O6rX_",
    "tags": []
   },
   "source": [
    "# Running InstructLab with a GPU\n",
    "\n",
    "<ul>\n",
    "<li>Contributors: InstructLab team and IBM Research Technology Education team:\n",
    "<li>Questions and support: kochel@us.ibm.com, IBM.Research.JupyterLab@ibm.com\n",
    "<li>Release date: 2025-07-03\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ics9GgZ-6rYB",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Overview\n",
    "This Jupyter notebook demonstrates InstructLab, an open source AI project that facilitates knowledge and skills contributions to Large Language Models (LLMs). InstructLab uses a novel synthetic data-based alignment tuning method for Large Language Models introduced in this [paper](https://arxiv.org/abs/2403.01081). The open source InstructLab repository is available [here](https://github.com/instructlab/instructlab) and provides additional documentation on using InstructLab.\n",
    "\n",
    "This notebook demonstrates the high level steps in the InstructLab processing, but does not run the full fidelity processing due to the limited GPU capability. The full InstructLab processing can be run via the [Red Hat AI InstructLab](https://cloud.ibm.com/instructlab/overview) service.\n",
    "\n",
    "InstructLab can take the form of an open source installation or a Red Hat AI InstructLab installation. In this notebook, we will demonstrate the open source version of InstructLab running on Colab with a GPU, broken into the following major sequential steps:\n",
    "\n",
    "1. Accepts one or more of Question and Answer (QNA) files as input\n",
    "1. Performs `yamllint` checks on the QNA files to verify their format\n",
    "1. Places the QNA files in the desired structure in a taxonomy\n",
    "1. Verifies the taxonomy by running the `ilab diff` function\n",
    "1. Generates synthetic data \n",
    "1. Trains with locally installed InstructLab\n",
    "1. Inferences with the newly trained model\n",
    "1. Downloads the trained model\n",
    "\n",
    "## Running this Notebook\n",
    "\n",
    "This notebook must be run within a Colab GPU runtime. You can check you are running with a GPU by selecting Runtime-> Change Runtime Type and confirming that a GPU Runtime is selected. While this notebook can be started on a free Colab account, the GPUs availabe with a free access do not have sufficient memory to run InstructLab training.\n",
    "\n",
    "You can run this notebook either:\n",
    "- Running All Cells by selecting Runtime->Run all\n",
    "- Cell by cell by selecting the arrow on each code cell and running them sequentially.\n",
    "\n",
    "Once the Configuring Instructlab section has been run, the other sections of this notebook can be repeatedly run on other data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OV_SZDBZgMVa"
   },
   "source": [
    "# Step 1. Clone the Instructlab Environment and Select Run Options\n",
    "\n",
    "The cell replicates an `ilab` data repository containing the pip requirements and data files, and then presents options for running the notebook.\n",
    "\n",
    "After selecting the parameters, the remainder of this notebook can be run by either:\n",
    "- Running all cells by selecting `Runtime`->`Run cell and below`.\n",
    "- Running each cell sequentially by clicking <img src=\"./refs/run-cell.png\" width=23> **Run cell** by each code cell.\n",
    "\n",
    "Run the following cell, select from the following parameters, and then follow the directions in the cell to run the rest of this notebook.\n",
    "\n",
    "We've provided question and qnswer files for these datasets:\n",
    "- \"2024 Oscar Awards Ceremony\"\n",
    "- \"Quantum Roadmap and Patterns\"\n",
    "- \"Artificial Intelligence Agents\"\n",
    "- \"Multi-QNA Example\": Contains QNA files for Oscars, Quantum, and Agentic AI data sets to show how multiple QNA files can be provided and processed.\n",
    "- \"Your Content 1\" or \"Your Content 2\": Follow the instructions in Step 2.2 to provide your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rde52-02CfiL"
   },
   "outputs": [],
   "source": [
    "#Install these itesm first to avoid a later reset\n",
    "!pip install psutil==7.0.0 pillow==10.4.0 numpy==1.26.4 torch==2.6.0 importlib_metadata==8.0.0 --quiet\n",
    "import os\n",
    "os.chdir('/content/')\n",
    "if not os.path.exists(\"ilab\"):\n",
    "    !git clone https://github.com/KenOcheltree/ilab-open.git --quiet --recurse-submodules ilab\n",
    "#Remove the colab sample_data\n",
    "if os.path.exists(\"sample_data\"):\n",
    "    !rm -rf sample_data\n",
    "\n",
    "#Display run options\n",
    "import ipywidgets as widgets\n",
    "#See instructions on placing your hf_token in colab userdata\n",
    "from google.colab import userdata\n",
    "hf_token=userdata.get('hf_token')\n",
    "data_set = widgets.ToggleButtons(\n",
    "    options=['2024 Oscars', 'Quantum', 'Agentic AI', 'Multi-QNA Example', 'Your Content 1', 'Your Content 2'],\n",
    "    description='Dataset:', style={\"button_width\": \"auto\"}\n",
    ")\n",
    "sdg_pipe = widgets.ToggleButtons(\n",
    "    options=['Simple', 'Full with GPU'], description='Processing:', style={\"button_width\": \"auto\"}\n",
    ")\n",
    "instr=widgets.ToggleButtons(\n",
    "    options=['Default (>450)','>15', '>50', '>200', '>500', '>1000'],\n",
    "    description='# of QNAs:', style={\"button_width\": \"auto\"}\n",
    ")\n",
    "train_pipe = widgets.ToggleButtons(\n",
    "    options=['Simple with GPU','Accelerated GPU'],description='Processing',style={\"button_width\":\"auto\"}\n",
    ")\n",
    "epoch=widgets.ToggleButtons(\n",
    "    options=['1', '2', '3', '4', '5', '10', '15'],description='Epochs:',style={\"button_width\":\"auto\"}\n",
    ")\n",
    "it=widgets.ToggleButtons(\n",
    "    options=['1', '3', '5','10','20','50','100','200'],description='Iterations:',style={\"button_width\":\"auto\"}\n",
    ")\n",
    "questions=widgets.ToggleButtons(options=['Yes','No'],description='Live Q&A:',style={\"button_width\":\"auto\"})\n",
    "download=widgets.ToggleButtons(options=['Yes','No'],description='Download:',style={\"button_width\":\"auto\"}\n",
    ")\n",
    "print(\"\\nSelect the Dataset for this run:\")\n",
    "display(data_set)\n",
    "print(\"Select the Synthetic Data Generation parameter to use:\")\n",
    "sdg_pipe.value='Simple'\n",
    "display(sdg_pipe)\n",
    "instr.value = 'Default (>450)'\n",
    "display(instr)\n",
    "print(\"Select the Training parameters to use:\")\n",
    "train_pipe.value='Simple with GPU'\n",
    "#display(train_pipe)\n",
    "epoch.value=\"3\"\n",
    "display(epoch)\n",
    "it.value=\"5\"\n",
    "display(it)\n",
    "print(\"Select what to do with the model after training:\")\n",
    "questions.value=\"Yes\"\n",
    "display(questions)\n",
    "download.value=\"No\"\n",
    "display(download)\n",
    "print(\"After selecting the parameters, select the next cell and then choose Runtime->Run cell and below\")\n",
    "print(\"When that run completes, you can come here, choose different parameters and rerun at the next cell with Runtime->Run cell and below\")\n",
    "print(\"Note: You can also go back and rerun individual sections of the notebook with different parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVAfX3Q5CfiL",
    "tags": []
   },
   "source": [
    "#Step 2. Prepare to Create the Taxonomy\n",
    "\n",
    "## 2.1 Provide the Taxonomy data\n",
    "\n",
    "You might want to run this notebook once with an existing data set before creating your own to understand the taxonomy creation flow.\n",
    "\n",
    "You can provide your own InstructLab QNA file for processing in this step.\n",
    "1. Create your own `qna.yaml` file by following the directions in the InstructLab taxonomy [readme](https://github.com/instructlab/taxonomy).\n",
    "1. After creating your `qna.yaml` file, add a comment in the first line that starts with `# Location:` and specifies the location of the file in the taxonomy. For example, a quantum computing `qna.yaml` file has the following path for the location:\n",
    "    ```\n",
    "    # location: /knowledge/information/computer_science/quantum_computing\n",
    "    ```\n",
    "1. Add your `qna.yaml` to the `/content/ilab/data/your_content_1` folder or the `/content/ilab/data/your_content_2` folder by dragging and dropping it into the folder.\n",
    "1. To include multiple `qna.yaml` files in your taxonomy, add a unique identifer `NNN` to the name so it is of the format `qnaNNN.yaml`. Any number of QNA files can be included as long as they have unique names.\n",
    "1. You can use your own data by selecting **Your Content 1** or **Your Content 2** in the code cell.\n",
    "\n",
    "##2.2 Complete the Environment Set Up\n",
    "\n",
    "This code cell installs the remainder of the required pip packages and configures InstructLab. The InstructLab configuration is captured in the `config.yaml` file. The `config.yaml` file is created for you and `taxomony_path = taxonomy` is set. The root location of the taxonomy is set to the taxonomy folder in `instructlab-latest`.\n",
    "\n",
    "**Note:** \n",
    "- This step can take a few minutes to run. If you are running all of the cells at the same time, it can take 10 minutes to run.\n",
    "- Ignore any pip inconsistency errors or warnings in the installation. They do not affect the running of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aFtTdSAbCfiL"
   },
   "outputs": [],
   "source": [
    "# Run the rest of the notebook by selecting this third cell and choosing \"Runtime->Run cell and below\"\n",
    "\n",
    "# Wrap Code cell output\n",
    "from IPython.display import HTML, display\n",
    "def set_css():\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)\n",
    "\n",
    "# Install the rest of the requirements\n",
    "!pip cache remove llama_cpp_python\n",
    "!pip install -r ilab/requirements_gpu.txt --quiet\n",
    "#Check the starting configuration\n",
    "!ilab system info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5EcJoSS6rYD",
    "tags": []
   },
   "source": [
    "#Step 3. Perform Imports and Check for a GPU\n",
    "\n",
    "This code cell checks for a GPU in the configuration. This notebook requires a GPU in the configuration to run properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kLDdSBiX6rYD"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from IPython.display import Image, display\n",
    "from datasets import load_dataset\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import json\n",
    "import subprocess\n",
    "import shutil\n",
    "import ruamel.yaml\n",
    "os.environ['NUMEXPR_MAX_THREADS'] = '64'\n",
    "Norm = \"<p style='font-family:IBM Plex Sans;font-size:20px'>\"\n",
    "\n",
    "notebook_dir='/content/ilab/'\n",
    "os.chdir(notebook_dir)\n",
    "\n",
    "## torch and cuda version check\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "\n",
    "if torch.cuda.is_available() is False:\n",
    "    print(\"No GPU in configuration\")\n",
    "else:\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    print(\"GPU(s) are Available\")\n",
    "    gpus=torch.cuda.device_count()\n",
    "    if gpus==1:\n",
    "      gpu_type=torch.cuda.get_device_name(0)\n",
    "      print(\"One GPU of Type: \", gpu_type)\n",
    "    else:\n",
    "      print(\"ERROR: More than 1 GPU in configuration: \",gpus)\n",
    "print(\"Starting directory: \"+ os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uv5uMvQ-a-ZF",
    "tags": []
   },
   "source": [
    "# Step 4. Configure the InstructLab Environment\n",
    "\n",
    "## 4.1 Run Instructlab initialization\n",
    "\n",
    "The InstructLab configuration is captured in the *config.yaml* file. This step creates the config.yaml file and sets:\n",
    "- **taxomony_path = taxonomy** - the root location of the taxonomy is set to the taxonomy folder in instructlab-latest\n",
    "- **model_path = models/merlinite-7b-lab-Q4_K_M.gguf** - the default model is set to merlinite\n",
    "\n",
    "**Note:** The default directories for InstructLab are the following. If you initialize InstructLab on your own system, it will default to the following:\n",
    "* **Downloaded Models:**  ~/.cache/instructlab/models/ - Contains all downloaded large language models, including the saved output of ones you generate with ilab.\n",
    "* **Synthetic Data:** ~/.local/share/instructlab/datasets/ - Contains data output from the SDG phase, built on modifications to the taxonomy repository.\n",
    "* **Taxonomy:** ~/.local/share/instructlab/taxonomy/ - Contains the skill and knowledge data.\n",
    "* **Training Output:** ~/.local/share/instructlab/checkpoints/ - Contains the output of the training process.\n",
    "* **config.yaml:** ~/.config/instructlab/config.yaml - Contains the config.yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QX9s4XZx6rYF"
   },
   "outputs": [],
   "source": [
    "#Remove Colab Sample directory\n",
    "if os.path.exists(\"sample_data\"):\n",
    "    print(\"removing sample_data\")\n",
    "    shutil.rmtree(\"sample_data\")\n",
    "    os.chdir(\"ilab\")\n",
    "\n",
    "#Initialize ilab\n",
    "base_dir=\"/root/\"\n",
    "##Choose the base model as granite or mixtral\n",
    "model_dir=\"models\"\n",
    "model_name=\"granite-7b-lab-Q4_K_M.gguf\"\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "\n",
    "taxonomy_path='taxonomy'\n",
    "\n",
    "## Define the file name\n",
    "file_name = \"config.yaml\"\n",
    "if os.path.exists(file_name):\n",
    "    os.remove(file_name)\n",
    "    print(f\"ilab was already initialized. {file_name} has been deleted. Reinitialized\")\n",
    "else:\n",
    "    print(f\"ilab was not initialized yet. {file_name} does not exist.\")\n",
    "\n",
    "##Remove old data\n",
    "if os.path.exists(\"taxonomy\"):\n",
    "    print(\"removing taxonomy\")\n",
    "    shutil.rmtree(\"taxonomy\")\n",
    "if os.path.exists(base_dir+\".cache/instructlab\"):\n",
    "    print(\"removing \" + base_dir+\".cache/instructlab\")\n",
    "    shutil.rmtree(base_dir+\".cache/instructlab\")\n",
    "if os.path.exists(base_dir+\".config/instructlab\"):\n",
    "    print(\"removing \" + base_dir+\".config/instructlab\")\n",
    "    shutil.rmtree(base_dir+\".config/instructlab\")\n",
    "if os.path.exists(base_dir+\".local/share/instructlab\"):\n",
    "    print(\"removing \" + base_dir+\".local/share/instructlab\")\n",
    "    shutil.rmtree(base_dir+\".local/share/instructlab\")\n",
    "\n",
    "print(f\"ilab model is {model_path}.\")\n",
    "print('#############################################################')\n",
    "print(' ')\n",
    "\n",
    "command = f\"\"\"\n",
    "ilab config init<<EOF\n",
    "{taxonomy_path}\n",
    "Y\n",
    "{model_path}\n",
    "0\n",
    "EOF\n",
    "\"\"\"\n",
    "\n",
    "## Using the ! operator to run the command\n",
    "!echo \"Running ilab config init\"\n",
    "!{command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8rbeRKE6rYF"
   },
   "source": [
    "#$ 4.2 Display the config.yaml file\n",
    "We examine the base configuration for identifying parameters for changing in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZflN-eeu6rYF",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "##to copy config.yaml to local directory\n",
    "!cp /root/.config/instructlab/config.yaml .\n",
    "!cat config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAL743VA6rYH"
   },
   "source": [
    "## 4.3 Select LLM Models and Download\n",
    "\n",
    "This cell changes the models to use for the generate stage. The mistral model as the teacher model in the generate step and as the student model to be trained.\n",
    "\n",
    "If you want to customize other models for generation or the training phase, you would specify the models in this step.\n",
    "\n",
    "This step specifies that the models to be used will be from this notebook's models directory.\n",
    "\n",
    "### LLM models\n",
    "The models that will be used in the InstructLab processing are downloaded in this step. Additional steps can be added if other models are used in processing.\n",
    "\n",
    "- The merlinite model will be used as the teacher model for the simple pipeline in the **Training with InstructLab** section.\n",
    "- The mistral-7b-instruct-v0.2.Q4_K_M model will be used as the teacher model for the full pipeline in that section.\n",
    "- The granite07b-lab.gguf model is a quantized version of the granite-7b-lab model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QILAQZYY6rYH",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "##Use ruamel.yaml to load the yaml file to preserve comments\n",
    "yaml = ruamel.yaml.YAML()\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.load(file)\n",
    "\n",
    "##Upate to use the same models and just change the directory\n",
    "teacher_model_path = \"models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n",
    "base_model_path = \"models/instructlab/granite-7b-lab\"\n",
    "##judge_model_path = \"models/prometheus-eval/prometheus-8x7b-v2.0\"\n",
    "\n",
    "##config['evaluate']['mt_bench']['judge_model'] = judge_model_path\n",
    "##config['evaluate']['mt_bench_branch']['judge_model'] = judge_model_path\n",
    "config['generate']['model'] = teacher_model_path\n",
    "config['generate']['teacher']['model_path']= teacher_model_path\n",
    "##config['train']['phased_mt_bench_judge']=judge_model_path\n",
    "\n",
    "#Update GPU information\n",
    "config['evaluate']['gpus']=gpus\n",
    "config['generate']['teacher']['vllm']['gpus']=gpus\n",
    "config['serve']['vllm']['gpus']=gpus\n",
    "config['train']['nproc_per_node']=gpus\n",
    "config['metadata']['gpu_count']=gpus\n",
    "if gpus==1:\n",
    "  config['train']['device']=\"cuda\"\n",
    "  if gpu_type[:6]==\"NVIDIA\":\n",
    "    config['metadata']['gpu_manufacturer']=\"Nvidia\"\n",
    "    config['metadata']['gpu_family']=gpu_type[7:]\n",
    "\n",
    "## Save the updated config.yaml file\n",
    "yaml.default_flow_style=False\n",
    "with open('config.yaml', 'w') as file:\n",
    "    yaml.dump(config, file)\n",
    "\n",
    "##copy the config file to the .config/instructlab/ where it is used by InstructLab\n",
    "!cp config.yaml {base_dir}.config/instructlab/\n",
    "\n",
    "print(\"Updated config.yaml successfully.\\n\")\n",
    "!cat config.yaml\n",
    "\n",
    "models_dir=\"models\"\n",
    "\n",
    "# Download models\n",
    "!ilab model download --hf-token {hf_token} --model-dir {models_dir}\n",
    "!ilab model download --repository instructlab/granite-7b-lab --hf-token {hf_token} --model-dir {models_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rCa2va8r6rYH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnOLAXRxvxgh",
    "tags": []
   },
   "source": [
    "# Step 5. Specify the Data for this Run\n",
    "\n",
    "We've provided question-and-answer files for these datasets: \"2024 Oscar Awards Ceremony\", \"Quantum Roadmap and Patterns\" and \"Artificial Intelligence Agents\". Feel free to choose one of these datasets, or select your own custom dataset in the cell below.\n",
    "\n",
    "### Optionally, Create your own data set for InstructLab\n",
    "\n",
    "You can optionally provide your own InstructLab QNA file for processing in this step.\n",
    "\n",
    "Follow these steps to add your own dataset:\n",
    "1. Create your own qna.yaml file following the directions on the InstructLab taxonomy [readme](https://github.com/instructlab/taxonomy).\n",
    "1. Create a questions.txt file with related sample questions to use on inferencing.\n",
    "1. Add your qna.yaml and sample questions.txt files to the /content/ilab/data/your_content_1 folder or the /content/ilab/data/your_content_2 folder by dragging and dropping them in the desired folder.\n",
    "1. Double click on the /content/ilab/config.json file to edit and specify the qna_location where your data resides within the Dewey Decimal classification system. Close and save the config.json file.\n",
    "1. You can now specify to run with your own data by selecting **Your Content 1** or **Your Content 2** in the next code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DbnDz_4j5GNi"
   },
   "outputs": [],
   "source": [
    "print(\"\\nSelect the QNA dataset to add:\")\n",
    "display(data_set)\n",
    "print(\"After choosing your dataset, please select and run the following cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzkSkC9OtxPO"
   },
   "source": [
    "# Step 6. Check the Format of the QNA YAML Files\n",
    "\n",
    "Running this cell checks the format of the yaml files before they are placed in the taxonomy to ensure they are the right length and there are no trailing blanks.\n",
    "\n",
    "Important: Rerun the following cell until all of the QNA files pass the yamllint test. Otherwise the file will fail in the Synthetic Data Generation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AxtkYOiJtwU2"
   },
   "outputs": [],
   "source": [
    "import yamllint\n",
    "# Select the folder of the dataset\n",
    "use_cases = {\"2024 Oscars\": \"oscars\", \"Quantum\": \"quantum\", \"Agentic AI\": \"agentic_ai\",\n",
    "            \"Multi-QNA Example\": \"example\",\"Your Content 1\": \"your_content_1\", \"Your Content 2\": \"your_content_2\"}\n",
    "use_case = use_cases[data_set.value]\n",
    "qna_dir = \"data/\" + use_case + \"/\"\n",
    "print(\"Running yaml checker on \" + data_set.value + \" data in folder \" + qna_dir)\n",
    "for f in os.listdir(qna_dir):\n",
    "    f=f.lower()\n",
    "    if f.startswith('qna'):\n",
    "      print(\"Checking File: \" + f)\n",
    "      yaml_file = qna_dir + f\n",
    "      shell_command = f\"yamllint /content/ilab/{yaml_file} -c /content/ilab/yamlrules.yaml\"\n",
    "      !{shell_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQBSnoObvxgh"
   },
   "source": [
    "# Step 7. Create the Taxonomy Data Repository\n",
    "Running this next cell places the QNA files in the proper directories of the taxonomy.\n",
    "\n",
    "If you want to add additional QNA files to the taxonomy after the following cell is run, you can create the necessary levels of directories and add the qna.yaml named file directly to the taxonomy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z2s_UhRCvxgh"
   },
   "outputs": [],
   "source": [
    "# List all of the files in the use_case directory that begin with QNA\n",
    "print_lines=30\n",
    "for f in os.listdir(qna_dir):\n",
    "    f=f.lower()\n",
    "    if f.startswith('qna'):\n",
    "        qna_file = qna_dir + f\n",
    "        print(\"Show the QNA file: \" + qna_file)\n",
    "        with open(qna_file, 'r') as input_file:\n",
    "            for line_number, line in enumerate(input_file):\n",
    "                if line_number == 0:\n",
    "                    words = line.split()\n",
    "                    print(\"Checking first line of QNA file for placement location: \" + line)\n",
    "                    if words[0] == \"#\" and words[1] == \"location:\" and len(words) == 3:\n",
    "                      qna_location = words[2]\n",
    "                    else:\n",
    "                      print(\"ERROR: Placement location not specified in QNA File: \" + qna_file)\n",
    "                      break\n",
    "                if line_number > print_lines:  # line_number starts at 0.\n",
    "                    break\n",
    "                print(line_number, line, end=\"\")\n",
    "        # Place the QNA file in the proper taxonomy directory if it does not already exist\n",
    "        new_qna_dir = \"/taxonomy\" + qna_location\n",
    "        if os.path.exists(os.getcwd()+new_qna_dir):\n",
    "            print(\"\\nWARNING: QNA file already exists in the taxonomy at duplicate location, not inserting\")\n",
    "        else:\n",
    "            print(\"\\nPlace QNA file in taxononmy as: /taxonomy\"+qna_location+\"/qna.yaml\")\n",
    "            shell_command1 = f\"mkdir -p ./taxonomy{qna_location}\"\n",
    "            shell_command2 = f\"cp ./{qna_file} ./taxonomy{qna_location}/qna.yaml\"\n",
    "            !{shell_command1}\n",
    "            !{shell_command2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qla2rqq4tpSK"
   },
   "source": [
    "# Step 8. Verify the Taxonomy Data Repository\n",
    "Run diff to verify the taxonomy. Record the errors on this step and correct them in your QNA files and then rerun the notebook with the corrected QNA files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nX7b5_pOtjj0"
   },
   "outputs": [],
   "source": [
    "print(\"Verify the taxonomy\")\n",
    "!ilab -vvv taxonomy diff --taxonomy-path taxonomy --taxonomy-base empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFKW-EIVvxgi",
    "tags": []
   },
   "source": [
    "# Step 9. Synthetic Data Generation\n",
    "\n",
    "## 9.1 Select parameters\n",
    "\n",
    "### Select pipeline\n",
    "\n",
    "InstructLab has three primary pipelines that can be used: simple, full and acellerated:\n",
    "- The **simple pipeline** runs fast and can be used for initial model and data testing.\n",
    "- The **full pipeline** runs all of the InstrctLab steps and takes more time but produces a better tuned model.\n",
    "\n",
    "**Note:** If you are running with a new or modifed dataset, you may want to use the **Simple pipeline** for the first run to verify the configuration\n",
    "\n",
    "### Select number of samples to generate\n",
    "\n",
    "Data generation takes 19 minutes for generating 15 synthetic data samples. You may wish to generate a small number on your first run to verify the QNA dataset format.\n",
    "\n",
    "To produce **sufficient synthetic data** to focus training on the new material, **about 30 synthetic questions and answer pairs need to be generated** for each question and answer pair provided. This will require a proportionally longer time to generate, but will provide better training.\n",
    "\n",
    "Before following these instructions, ensure the existing model you are adding skills or knowledge to is still running. Alternatively, ilab data generate can start a server for you if you provide a fully qualified model path via --model.\n",
    "\n",
    "To generate a synthetic dataset based on your newly added knowledge or skill set in taxonomy repository, run the following command:\n",
    "\n",
    "    ilab data generate\n",
    "\n",
    "### **Simple Pipeline**\n",
    "\n",
    "The Simple Pipeline works solely with Merlinite 7b Lab as the teacher model. The Simple Pipeline is called without GPU acceleration as follows:\n",
    "\n",
    "    ilab data generate --pipeline simple\n",
    "\n",
    "### **Full Pipeline**\n",
    "\n",
    "The Full Pipeline runs the full processing with a GPU. Currently, the Full Pipeline only supports the Mixtral and Mistral Instruct Family models as the teacher model.  This is due to only supporting specific model prompt templates.\n",
    "\n",
    "Using a non-default model such as Mixtral-8x7B-Instruct-v0.1) to generate data with the Full Pipeline:\n",
    "\n",
    "    ilab data generate --model ~/.cache/instructlab/models/mistralai/mixtral-8x7b-instruct-v0.1 --pipeline full --gpus 4\n",
    "\n",
    "**Note** Synthetic Data Generation can take from 2 minutes to 1+ hours to complete, depending on your computing resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8rNYESyEvxgi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Select Pipeline to use\")\n",
    "display(sdg_pipe)\n",
    "display(instr)\n",
    "print(\"After making your selections for data generation, please select and run the following cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-so0dOw4vxgi",
    "tags": []
   },
   "source": [
    "## 9.2 Run data generation\n",
    "Data generation with a GPU can take 2 minutes or more to generate 15 synthetic data samples. It takes proportionately longer to generate more samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wn6gODEcvxgi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen_directory = \"data/\"+ use_case+\"/ilab_generated/\"\n",
    "if instr.value == \"Default (>450)\":\n",
    "        sdg_factor=\"\"\n",
    "elif instr.value == '>15':\n",
    "    sdg_factor=\"--sdg-scale-factor 1\"\n",
    "elif instr.value == '>50':\n",
    "    sdg_factor=\"--sdg-scale-factor 3\"\n",
    "elif instr.value == '>200':\n",
    "    sdg_factor=\"--sdg-scale-factor 13\"\n",
    "elif instr.value == '>500':\n",
    "    sdg_factor=\"--sdg-scale-factor 33\"\n",
    "else:\n",
    "    sdg_factor=\"--sdg-scale-factor 67\"\n",
    "# 'Fast (Simple)', 'Full with CPU'\n",
    "if sdg_pipe.value == 'Simple':\n",
    "    pipeline = 'simple'\n",
    "    model = '--model models/instructlab/granite-7b-lab'\n",
    "    gpus = '--gpus 1'\n",
    "elif sdg_pipe.value == 'Full with GPU':\n",
    "    pipeline = 'full'\n",
    "    model = ''\n",
    "#   model = '--model models/instructlab/granite-7b-lab'\n",
    "    gpus = '--gpus 1'\n",
    "else:\n",
    "    print(\"ERROR: Undefined pipeline\")\n",
    "\n",
    "il_data_path= '/root/.local/share/instructlab/datasets/'\n",
    "#Remove old data so there is only one test_merlinite and train_merlinite after generation\n",
    "print(\"Remove old datasets\")\n",
    "!rm -rf {il_data_path}*\n",
    "#shell_command = f\"ilab --verbose data generate {model} --num-cpus 10 {gpus} {sdg_factor} --taxonomy-path taxonomy --pipeline {pipeline} --max-num-tokens 512\"\n",
    "shell_command = f\"ilab data generate {model} --num-cpus 10 {gpus} {sdg_factor} --taxonomy-path taxonomy --pipeline {pipeline} --max-num-tokens 512\"\n",
    "\n",
    "print(\"Generating data\")\n",
    "print(\"Running: !\"+shell_command)\n",
    "!{shell_command}\n",
    "\n",
    "#Rename results to  test_gen.jsonl and train_gen.jsonl and move to local data directory\n",
    "if not os.path.exists(gen_directory):\n",
    "    print(\"Create directory: \" + gen_directory)\n",
    "    !mkdir {gen_directory}\n",
    "file_cnt=0\n",
    "try:\n",
    "    for dirname in os.listdir(il_data_path):\n",
    "        date_path=il_data_path+'/'+ dirname + '/'\n",
    "        for filename in os.listdir(date_path):\n",
    "            if filename[:6]=='train_':\n",
    "                train_name= 'train_gen.jsonl'\n",
    "                print('Renaming '+ filename+ ' to ' + train_name)\n",
    "                !mv {date_path+filename} {gen_directory+train_name}\n",
    "                file_cnt+=1\n",
    "            elif filename[:5]=='test_':\n",
    "                test_name= 'test_gen.jsonl'\n",
    "                print('Renaming '+ filename+ ' to ' + test_name)\n",
    "                !mv {date_path+filename} {gen_directory+test_name}\n",
    "                file_cnt+=1\n",
    "    if file_cnt < 2:\n",
    "        print(\"ERROR: train_gen.jsonl and/or test.jsonl not created\")\n",
    "    elif os.path.getsize(gen_directory+train_name) == 0:\n",
    "        print(\"ERROR: train_gen.jsonl file is empty\")\n",
    "    elif os.path.getsize(gen_directory+test_name) == 0:\n",
    "        print(\"ERROR: test_gen.jsonl file is empty\")\n",
    "    else:\n",
    "        print(\"Training and test files successfully created in: \" + gen_directory)\n",
    "except:\n",
    "    print(\"Error running ilab generate, no synthetic data generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0B_39Sqvxgj"
   },
   "source": [
    "## 9.3 Show examples of generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLqnVW7rvxgj"
   },
   "outputs": [],
   "source": [
    "print(\"2.4.3 Show examples of generated data\")\n",
    "for filename in os.listdir(gen_directory):\n",
    "    if filename[:9]=='train_gen':\n",
    "        with open(gen_directory+filename, 'r') as syn_file:\n",
    "            cnt=0\n",
    "            for line_number, line in enumerate(syn_file):\n",
    "                if cnt >= 8:\n",
    "                    break\n",
    "                jsonLine= json.loads(line)\n",
    "                syn_user=jsonLine[\"user\"]\n",
    "                syn_assist=jsonLine[\"assistant\"]\n",
    "                #Remove \"Answer:\" and \"Response:\" from answers for displaying\n",
    "                if syn_user[:10]==\"Question: \":\n",
    "                    syn_user=syn_user[10:]\n",
    "                if syn_assist[:8]==\"Answer: \":\n",
    "                    syn_assist=syn_assist[8:]\n",
    "                cnt+=1\n",
    "                print(\"\\nQuestion: \"+syn_user+\"\\nAnswer: \"+syn_assist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvTYe8oHdm13",
    "tags": []
   },
   "source": [
    "# Step 10. Train with InstructLab\n",
    "\n",
    "## 10.1 Select the model training pipeline\n",
    "\n",
    "InstructLab has three primary model training pipelines: simple, full (default), and accelerated. For all of the models, the training time can be limited by adjusting the num_epoch paramater. The maximum number of epochs for running the InstructLab end-to-end workflow is 10.\n",
    "\n",
    "#### **Simple pipeline**\n",
    "\n",
    "The simple pipeline uses an SFT Trainer on Linux and MLX on MacOS. This type of training takes roughly an hour and produces the lowest fidelity model but should indicate if your data is being picked up by the training process. The simple pipeline only works with Merlinite 7b Lab as the teacher model. For this Linux system, the trained model is saved in the models directory as ggml-model-f16.gguf.\n",
    "\n",
    "The command form is:\n",
    "\n",
    "    ilab model train --pipeline simple\n",
    "\n",
    "**Note:** This process will take a little while to complete (time can vary based on hardware and output of ilab data generate but on the order of 5 to 15 minutes)\n",
    "\n",
    "#### **Accelerated pipeline**\n",
    "\n",
    "The accelerated uses the instructlab-training library which supports GPU accelerated and distributed training. The full loop and data processing functions are either pulled directly from or based off of the work in this library. For the accelerated pipeline, the models are saved in the ~/.local/share/instructlab/checkpoints directory. The instructlab command \"ilab model evaluate\" can be used to choose the best one. Training is support for GPU acceleration with Nvidia CUDA or AMD ROCm. Please see the GPU acceleration documentation for more details. At present, hardware acceleration requires a data center GPU or high-end consumer GPU with at least 18 GB free memory.\n",
    "\n",
    "The command form is:\n",
    "\n",
    "    ilab model train --pipeline accelerated --device cuda --data-path <path-to-sdg-data>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8UrXh9-nvxgj"
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Select to Continue or to Train the model\")\n",
    "display(train_pipe)\n",
    "display(epoch)\n",
    "display(it)\n",
    "print(\"After choosing your training options, please select and run the following cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUPA7SpTvxgj"
   },
   "source": [
    "## 10.2 Run the model training\n",
    "\n",
    "Model training can take 30 minutes or more for 1 epoch and 1 iteration and takes 1 hour for the default paramter values. This minimal training could be used for testing the generation and training for a new set of data.\n",
    "\n",
    "To produce a higher quality model, more epochs and iterations are needed for refining the model. This will require a proportionally longer time to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LcIjCQUjvxgj",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path=\"data/\"+ use_case+\"/ilab_generated/\"\n",
    "train_data=data_path+\"train_gen.jsonl\"\n",
    "model_path=\"models/instructlab/granite-7b-lab\"\n",
    "##model_path='/root/.cache/instructlab/models/instructlab/granite-7b-lab'\n",
    "trained_model_path=\"data/\"+ use_case+\"/new_model/\"\n",
    "\n",
    "##'Simple (Fast)', 'Accelerated GPU'\n",
    "file_cnt=0\n",
    "for filename in os.listdir(data_path):\n",
    "    if filename[:15]=='train_gen.jsonl': file_cnt+=1\n",
    "    elif filename[:14]=='test_gen.jsonl': file_cnt+=1\n",
    "if file_cnt < 2 or os.path.getsize(gen_directory+train_name) < 5 or os.path.getsize(gen_directory+test_name) < 5:\n",
    "    print(\"ERROR: train_gen.jsonl and/or test.jsonl are not present or too small\")\n",
    "\n",
    "if not os.path.exists(trained_model_path):\n",
    "    print(\"Create directory: \" + trained_model_path)\n",
    "    !mkdir {trained_model_path}\n",
    "ep=int(epoch.value)\n",
    "its=int(it.value)\n",
    "if train_pipe.value=='Simple with GPU':\n",
    "    print(\"Train with simple pipeline with a GPU\")\n",
    "    shell_command = f\"ilab model train --pipeline simple --device cuda --model-path {model_path} --data-path {data_path} --num-epochs {ep} --iters {its}\"\n",
    "elif train_pipe.value=='Accelerated GPU':\n",
    "    print(\"Train accelerated with a GPU\")\n",
    "    #shell_command = f\"ilab model train --pipeline accelerated --device cuda --model-path {model_path} --data-path {train_data} --num-epochs {ep} --iters {its}\"\n",
    "    shell_command = f\"ilab -v -v model train --pipeline accelerated --device cuda --model-path {'/content/ilab/'+model_path} --data-path {'/content/ilab/'+train_data}\"\n",
    "\n",
    "print(\"Running: !\"+shell_command)\n",
    "!{shell_command}\n",
    "if train_pipe.value=='Accelerated GPU':\n",
    "    print(\"Run ilab model evaluate\")\n",
    "    !ilab model evaluate --benchmark mmlu\n",
    "#Move the model to the use_case/new_model directory\n",
    "print(\"Moving the trained model to the directory: \"+trained_model_path)\n",
    "!mv /root/.local/share/instructlab/checkpoints/ggml-model-f16.gguf {trained_model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SogwbM1vxgk",
    "tags": []
   },
   "source": [
    "# Step 11. Utilize the Newly Trained Model\n",
    "\n",
    "## 11.1 Run Interactive Q&A Session with Base and Trained Models to Evaluate Performance\n",
    "\n",
    "You have now completed InstructLab training. You can run this section to ask questions to both the base and InstructLab trained models and to compare answers.\n",
    "\n",
    "Run both base and trained models to compare results with interactive questions and and answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BoD1Ngrv7ni3"
   },
   "outputs": [],
   "source": [
    "print(\"Do you want to run interactive question on the base and trained models?\")\n",
    "display(questions)\n",
    "print(\"After making your choice, please select and run the following cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AK5l-lIE7oPF"
   },
   "source": [
    "The following are sample questions derived from the data used to generate synthetic data, which was then employed to train the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gVp-BeByvxgl"
   },
   "outputs": [],
   "source": [
    "# Define a Function to Perform Inference on Base and Trained Models\n",
    "def model_inference(base_model_path, trained_model_path):\n",
    "    _DEFAULT_TEMPLATE = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "    Current conversation:\n",
    "    Human: {input}\n",
    "    AI:\"\"\"\n",
    "    base_llm = LlamaCpp(model_path=base_model_path,\n",
    "                   verbose=False,\n",
    "                   n_gpu_layers=25,\n",
    "                   max_tokens=90,\n",
    "                   temperature=0,\n",
    "                   top_k=1\n",
    "                  )\n",
    "    trained_llm = LlamaCpp(model_path=trained_model_path,\n",
    "                   verbose=False,\n",
    "                   n_gpu_layers=25,\n",
    "                   max_tokens=90,\n",
    "                   temperature=0,\n",
    "                   top_k=1\n",
    "                  )\n",
    "    PROMPT = PromptTemplate( input_variables=[\"input\"],\n",
    "                            template=_DEFAULT_TEMPLATE\n",
    "                            )\n",
    "    chain1 = PROMPT | base_llm | StrOutputParser()\n",
    "    chain2 = PROMPT | trained_llm | StrOutputParser()\n",
    "    while True:\n",
    "        question = input(\"Ask me a question (type 'exit' to end): \")\n",
    "        if question.lower() == 'exit':\n",
    "            print(\"Exiting this Q&A session.\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"You asked: \", question)\n",
    "            answer1 = chain1.invoke(question)\n",
    "            answer1= answer1.split('Human',1)[0]\n",
    "            print (\"Base Model Answer: \",answer1)\n",
    "            answer2 = chain2.invoke(question)\n",
    "            answer2= answer2.split('Human',1)[0]\n",
    "            print (\"Trained Model Answer: \",answer2)\n",
    "\n",
    "##Display Sample Questions\n",
    "base_model = notebook_dir +\"/models/granite-7b-lab-Q4_K_M.gguf\"\n",
    "trained_model = trained_model_path + \"ggml-model-f16.gguf\"\n",
    "if questions.value=='Yes':\n",
    "  with open(notebook_dir+'/data/' + use_case + '/questions.txt') as f:\n",
    "      for line in f.readlines():\n",
    "          display(widgets.HTML(Norm+line))\n",
    "  print(\"Processing may take several minutes on the first run...\")\n",
    "  model_inference(base_model, trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5l5m7L_LDV1"
   },
   "source": [
    "## 11.2 Download the Newly Trained Model\n",
    "### Show download option\n",
    " Now that we have a model trained on our dataset, we can download the trained model for futher testing and use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kx65h6u17SW_"
   },
   "outputs": [],
   "source": [
    "print(\"Do you want to download the trained model to your local machine?\")\n",
    "display(download)\n",
    "print(\"After making your selection, please select and run the following cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwwZ4P6n7TDu"
   },
   "source": [
    "### Download the model\n",
    "Select and run the next cell to download if selected. Please make sure the download is completed before closing this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZsjOChUK7gj"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "if download.value=='Yes':\n",
    "  files.download(trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63hBXXa0vxgl",
    "tags": []
   },
   "source": [
    "<a id=\"IL3_learn\"></a>\n",
    "# Learn More\n",
    "This notebook demonstrates the high level steps in the InstructLab processing, but does not run the full fidelity processing due to the limited GPU capability. The full InstructLab processing can be run via the [Red Hat AI InstructLab](https://cloud.ibm.com/instructlab/overview) service.\n",
    "\n",
    "InstructLab uses a novel synthetic data-based alignment tuning method for Large Language Models introduced in this [paper](https://arxiv.org/abs/2403.01081).\n",
    "\n",
    "This notebook is based on the InstructLab CLI repository available [here](https://github.com/instructlab/instructlab).\n",
    "\n",
    "Contact us by email to ask questions, discuss potential use cases, or schedule a technical deep dive. The contact email is IBM.Research.JupyterLab@ibm.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "poB7nDmcvxgl"
   },
   "source": [
    "Â© 2025 IBM Corporation"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
