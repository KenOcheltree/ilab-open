{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7ih7e5O6rX_",
        "tags": []
      },
      "source": [
        "# Running InstructLab with a GPU\n",
        "\n",
        "<ul>\n",
        "<li>Contributors: InstructLab team and IBM Research Technology Education team\n",
        "<li>Contact for questions and technical support: IBM.Research.JupyterLab@ibm.com\n",
        "<li>Provenance: IBM Research\n",
        "<li>Version: 1.0.9\n",
        "<li>Release date: 2024-2-7\n",
        "<li>Compute requirements: CPU\n",
        "<li>Memory requirements: 16 GB\n",
        "<li>Notebook set: InstructLab\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ics9GgZ-6rYB",
        "tags": []
      },
      "source": [
        "## Summary\n",
        "This notebook set demonstrates InstructLab, an open source AI project that facilitates knowledge and skills contributions to Large Language Models (LLMs). InstructLab uses a novel synthetic data-based alignment tuning method for Large Language Models introduced in this [paper](https://arxiv.org/abs/2403.01081). The open source InstructLab repository is available [here](https://github.com/instructlab/instructlab) and provides additional documentation on using InstructLab.\n",
        "\n",
        "<img src=\"https://github.com/KenOcheltree/ilab-colab/blob/main/data/images/Flow.png?raw=1\" width=\"1000\">\n",
        "\n",
        "InstructLab  can be instantiated in several different forms, depending on the processing capabilities available. InstructLab can take the form of an open source installation or a Red Hat AI InstructLab installation. The open source installation can be run on a range of hardware from a laptop to a build your own (BYO) server instance running on a Virtual Machine (VM). The below figure shows the different available instantiations of InstructLab.\n",
        "\n",
        "<img src=\"https://github.com/KenOcheltree/ilab-colab/blob/main/data/images/experiences.png?raw=1\" width=\"1000\">\n",
        "\n",
        "In this notebook set, we will be demonstrating both the open source version running on a VM server and Red Hat Enterprise Linux AI InstructLab running on an IBM Cloud Server.\n",
        "\n",
        "The open source version running on a server is demonstrated in the following sections that are run sequentially:\n",
        "* [Configuring InstructLab](#IL1_0)\n",
        "* [Training with InstructLab](#IL2_0)\n",
        "* [Inferencing with InstructLab](#IL3_0)\n",
        "\n",
        "The Red Hat Enterprise Linux AI InstructLab is demonstrated running as a service in the IBM Cloud by running the following notebooks:\n",
        "- Configuring InstructLab\n",
        "- Training with Red Hat AI InstructLab Service\n",
        "- Inferencing with Redhat-AI-InstructLab Trained Model\n",
        "**Note:** The **Configuring InstructLab** notebook is run before the other Red Hat AI InstructLab notebooks to ensure that the *granite 7b model* is installed for inference comparisons as the base model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnQ88KKN6rYC",
        "tags": []
      },
      "source": [
        "<a id=\"IL0_0\"></a>\n",
        "# 1. Configuring InstructLab\n",
        "\n",
        "This notebook demonstrates the configuration of InstructLab. The InstructLab method consists of three major components:\n",
        "* **Taxonomy-driven data curation:**  The taxonomy is a set of training data curated by humans as examples of new knowledge and skills for the model.\n",
        "* **Large-scale synthetic data generation:** A teacher model is used to generate new examples based on the seed training data. Recognizing that synthetic data can vary in quality, the InstructLab method adds an automated step to refine the example answers, ensuring they are grounded and safe.\n",
        "* **Iterative model alignment tuning:** The model is retrained based on the synthetic data. The InstructLab method includes two tuning phases: knowledge tuning, followed by skill tuning.\n",
        "\n",
        "In this notebook, we will demonstrate the following steps:\n",
        "\n",
        "* [Step 0. Environment Preconfiguration](#IL1_preconfig)\n",
        "* [Step 1. Check the Starting Configuration](#IL1_init)\n",
        "* [Step 2. Configure InstructLab](#IL1_config)\n",
        "* [Step 3. Download Models](#IL1_down)\n",
        "\n",
        "**Note:** This notebook must be run within a GPU session. If you are not running with a GPU, please select **File->Hub Control Panel->Stop My Server**, then **Start My Server** and then select a GPU Session."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlk9FjFB6rYD",
        "tags": []
      },
      "source": [
        "<a id=\"IL1_preconfig\"></a>\n",
        "## Step 1.0 Environment Preconfiguration\n",
        "This step has already been performed for this JupyterLab environment and requires no work by the user. This information is provided in case the user is setting up their own environment on their own laptop or server.\n",
        "\n",
        "The full steps for a direct installation are [here](https://github.com/instructlab/instructlab).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDy_E7vyvxga",
        "outputId": "ed98272f-d6b4-438e-f8cc-b32c3b9c07de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.11\n",
            "Cloning into 'ilab'...\n",
            "remote: Enumerating objects: 55, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "remote: Total 55 (delta 13), reused 37 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (55/55), 1.01 MiB | 3.15 MiB/s, done.\n",
            "Resolving deltas: 100% (13/13), done.\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!git clone https://github.com/KenOcheltree/ilab.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "liQGtCnW1ROw",
        "outputId": "81468829-d15e-4381-af44-fb7c52397bd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: No matching packages for pattern \"llama_cpp_python\"\u001b[0m\u001b[33m\n",
            "\u001b[0mFiles removed: 0\n"
          ]
        }
      ],
      "source": [
        "!pip cache remove llama_cpp_python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2WZ1WaL1ROw"
      },
      "outputs": [],
      "source": [
        "#!pip install rouge-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTFyjQ1V1ROx"
      },
      "outputs": [],
      "source": [
        "#!pip install deepspeed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oVIPYUa1ROx"
      },
      "outputs": [],
      "source": [
        "#!pip install flash-attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYE2zACd1ROx"
      },
      "outputs": [],
      "source": [
        "#!pip install sqlitedict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oogIsvhG1ROy"
      },
      "outputs": [],
      "source": [
        "#!pip install word2number"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vllm==0.6.2"
      ],
      "metadata": {
        "id": "WUqGJYNx1Xks",
        "outputId": "2e745d4e-b462-4d4e-d782-16b0fb7b4800",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vllm==0.6.2\n",
            "  Downloading vllm-0.6.2-cp38-abi3-manylinux1_x86_64.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.2) (5.9.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.2) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.2) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.2) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.2) (4.67.1)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.2) (9.0.0)\n",
            "Requirement already satisfied: transformers>=4.45.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.2) (4.48.3)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.2) (0.21.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.2) (4.25.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.2) (3.11.12)\n",
            "Requirement already satisfied: openai>=1.40.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.2) (1.61.1)\n",
            "Collecting uvicorn[standard] (from vllm==0.6.2)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.2) (2.10.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.2) (11.1.0)\n",
            "Requirement already satisfied: prometheus-client>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.2) (0.21.1)\n",
            "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm==0.6.2)\n",
            "  Downloading prometheus_fastapi_instrumentator-7.0.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting tiktoken>=0.6.0 (from vllm==0.6.2)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting lm-format-enforcer==0.10.6 (from vllm==0.6.2)\n",
            "  Downloading lm_format_enforcer-0.10.6-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting outlines<0.1,>=0.0.43 (from vllm==0.6.2)\n",
            "  Downloading outlines-0.0.46-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.10 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.2) (4.12.2)\n",
            "Requirement already satisfied: filelock>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.2) (3.17.0)\n",
            "Collecting partial-json-parser (from vllm==0.6.2)\n",
            "  Downloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.2) (24.0.1)\n",
            "Collecting msgspec (from vllm==0.6.2)\n",
            "  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting gguf==0.10.0 (from vllm==0.6.2)\n",
            "  Downloading gguf-0.10.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.2) (8.6.1)\n",
            "Collecting mistral-common>=1.4.3 (from vllm==0.6.2)\n",
            "  Downloading mistral_common-1.5.3-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.2) (6.0.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from vllm==0.6.2) (0.8.1)\n",
            "Collecting ray>=2.9 (from vllm==0.6.2)\n",
            "  Downloading ray-2.42.1-cp311-cp311-manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting nvidia-ml-py (from vllm==0.6.2)\n",
            "  Downloading nvidia_ml_py-12.570.86-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting torch==2.4.0 (from vllm==0.6.2)\n",
            "  Downloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchvision==0.19 (from vllm==0.6.2)\n",
            "  Downloading torchvision-0.19.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
            "Collecting xformers==0.0.27.post2 (from vllm==0.6.2)\n",
            "  Downloading xformers-0.0.27.post2-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting fastapi>=0.114.1 (from vllm==0.6.2)\n",
            "  Downloading fastapi-0.115.8-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting interegular>=0.3.2 (from lm-format-enforcer==0.10.6->vllm==0.6.2)\n",
            "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lm-format-enforcer==0.10.6->vllm==0.6.2) (24.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->vllm==0.6.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->vllm==0.6.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->vllm==0.6.2) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->vllm==0.6.2) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0->vllm==0.6.2)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0->vllm==0.6.2)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0->vllm==0.6.2)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0->vllm==0.6.2)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0->vllm==0.6.2)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0->vllm==0.6.2)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0->vllm==0.6.2)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0->vllm==0.6.2)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0->vllm==0.6.2)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0->vllm==0.6.2)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0->vllm==0.6.2)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.0.0 (from torch==2.4.0->vllm==0.6.2)\n",
            "  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0->vllm==0.6.2) (12.5.82)\n",
            "Collecting starlette<0.46.0,>=0.40.0 (from fastapi>=0.114.1->vllm==0.6.2)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.11/dist-packages (from mistral-common>=1.4.3->vllm==0.6.2) (4.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.40.0->vllm==0.6.2) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.40.0->vllm==0.6.2) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.40.0->vllm==0.6.2) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.40.0->vllm==0.6.2) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.40.0->vllm==0.6.2) (1.3.1)\n",
            "Collecting lark (from outlines<0.1,>=0.0.43->vllm==0.6.2)\n",
            "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.2) (1.6.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.2) (3.1.1)\n",
            "Collecting diskcache (from outlines<0.1,>=0.0.43->vllm==0.6.2)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.2) (0.61.0)\n",
            "Requirement already satisfied: referencing in /usr/local/lib/python3.11/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.2) (0.36.2)\n",
            "Collecting datasets (from outlines<0.1,>=0.0.43->vllm==0.6.2)\n",
            "  Downloading datasets-3.3.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting pycountry (from outlines<0.1,>=0.0.43->vllm==0.6.2)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pyairports (from outlines<0.1,>=0.0.43->vllm==0.6.2)\n",
            "  Downloading pyairports-2.1.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm==0.6.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->vllm==0.6.2) (2.27.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray>=2.9->vllm==0.6.2) (8.1.8)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray>=2.9->vllm==0.6.2) (1.1.0)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray>=2.9->vllm==0.6.2) (1.3.2)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray>=2.9->vllm==0.6.2) (1.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->vllm==0.6.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->vllm==0.6.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->vllm==0.6.2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->vllm==0.6.2) (2025.1.31)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.6.0->vllm==0.6.2) (2024.11.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.19.1->vllm==0.6.2) (0.28.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.45.0->vllm==0.6.2) (0.5.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm==0.6.2) (2.4.6)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm==0.6.2) (25.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm==0.6.2) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm==0.6.2) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->vllm==0.6.2) (1.18.3)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->vllm==0.6.2) (3.21.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm==0.6.2) (0.14.0)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]->vllm==0.6.2)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]->vllm==0.6.2)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]->vllm==0.6.2)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]->vllm==0.6.2)\n",
            "  Downloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]->vllm==0.6.2) (14.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>=1.40.0->vllm==0.6.2) (1.0.7)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral-common>=1.4.3->vllm==0.6.2) (2024.10.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral-common>=1.4.3->vllm==0.6.2) (0.22.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.2) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.2)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.2) (2.2.2)\n",
            "Collecting xxhash (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.2)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.2)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.4.0->vllm==0.6.2) (3.0.2)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->outlines<0.1,>=0.0.43->vllm==0.6.2) (0.44.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.4.0->vllm==0.6.2) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm==0.6.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm==0.6.2) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm==0.6.2) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->outlines<0.1,>=0.0.43->vllm==0.6.2) (1.17.0)\n",
            "Downloading vllm-0.6.2-cp38-abi3-manylinux1_x86_64.whl (228.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.3/228.3 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gguf-0.10.0-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lm_format_enforcer-0.10.6-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl (797.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.3/797.3 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.19.0-cp311-cp311-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m116.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.27.post2-cp311-cp311-manylinux2014_x86_64.whl (20.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.8-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mistral_common-1.5.3-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m114.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outlines-0.0.46-py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.9/101.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prometheus_fastapi_instrumentator-7.0.2-py3-none-any.whl (18 kB)\n",
            "Downloading ray-2.42.1-cp311-cp311-manylinux2014_x86_64.whl (67.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.4/67.4 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_ml_py-12.570.86-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl (10 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.3.1-py3-none-any.whl (484 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m484.9/484.9 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyairports-2.1.1-py3-none-any.whl (371 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m371.7/371.7 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m113.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyairports, nvidia-ml-py, xxhash, uvloop, uvicorn, triton, python-dotenv, pycountry, partial-json-parser, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, msgspec, lark, interegular, httptools, gguf, diskcache, dill, watchfiles, tiktoken, starlette, nvidia-cusolver-cu12, nvidia-cudnn-cu12, multiprocess, torch, prometheus-fastapi-instrumentator, lm-format-enforcer, fastapi, xformers, torchvision, ray, mistral-common, datasets, outlines, vllm\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu124\n",
            "    Uninstalling torch-2.5.1+cu124:\n",
            "      Successfully uninstalled torch-2.5.1+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.20.1+cu124\n",
            "    Uninstalling torchvision-0.20.1+cu124:\n",
            "      Successfully uninstalled torchvision-0.20.1+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.3.1 dill-0.3.8 diskcache-5.6.3 fastapi-0.115.8 gguf-0.10.0 httptools-0.6.4 interegular-0.3.3 lark-1.2.2 lm-format-enforcer-0.10.6 mistral-common-1.5.3 msgspec-0.19.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-ml-py-12.570.86 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 outlines-0.0.46 partial-json-parser-0.2.1.1.post5 prometheus-fastapi-instrumentator-7.0.2 pyairports-2.1.1 pycountry-24.6.1 python-dotenv-1.0.1 ray-2.42.1 starlette-0.45.3 tiktoken-0.9.0 torch-2.4.0 torchvision-0.19.0 triton-3.0.0 uvicorn-0.34.0 uvloop-0.21.0 vllm-0.6.2 watchfiles-1.0.4 xformers-0.0.27.post2 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama_cpp_python[server]==0.3.2 -C cmake.args='-DLLAMA_CUDA=on'"
      ],
      "metadata": {
        "id": "mfvhujTK1c-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqPdN3XB1ROy"
      },
      "outputs": [],
      "source": [
        "!pip install instructlab==0.24.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_TTU5vUhXvM",
        "tags": []
      },
      "source": [
        "### Check InstructLab Version\n",
        "\n",
        "Check that InstructLab version 0.24.0 is installed properly and is configured for using a GPU.\n",
        "\n",
        "The first line from 'InstructLab' section should read\n",
        "```\n",
        "instructlab.version: 0.24.0\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqjqIGE06rYE"
      },
      "outputs": [],
      "source": [
        "!ilab system info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5EcJoSS6rYD",
        "tags": []
      },
      "source": [
        "<a id=\"IL1_check\"></a>\n",
        "## Step 1.1 Check for a GPU\n",
        "\n",
        "This code cell checks for a GPU in the configuration. This notebook requires a GPU in the configuration to run properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLDdSBiX6rYD"
      },
      "outputs": [],
      "source": [
        "## standard imports\n",
        "import os\n",
        "import torch\n",
        "from IPython.display import Image, display\n",
        "import ipywidgets as widgets\n",
        "\n",
        "H1 = \"<p style='font-family:IBM Plex Sans;font-size:28px'>\"\n",
        "H2 = \"<p style='font-family:IBM Plex Sans;font-size:24px'>\"\n",
        "Norm = \"<p style='font-family:IBM Plex Sans;font-size:20px'>\"\n",
        "Small = \"<p style='font-family:IBM Plex Sans;font-size:17px'>\"\n",
        "Ex = \"<p style='font-family:IBM Plex Sans;font-size:20px;font-style:italic'>\"\n",
        "\n",
        "## torch and cuda version check\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "\n",
        "if torch.cuda.is_available() is False:\n",
        "    print(\"No GPU in configuration\")\n",
        "else:\n",
        "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "    print(\"GPU is Available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv5uMvQ-a-ZF",
        "tags": []
      },
      "source": [
        "<a id=\"IL1_config\"></a>\n",
        "## Step 1.2 Configure InstructLab\n",
        "\n",
        "### 1.2.1 Create InstructLab config file\n",
        "The InstructLab configuration is captured in the *config.yaml* file. This step creates the config.yaml file and sets:\n",
        "- **taxomony_path = taxonomy** - the root location of the taxonomy is set to the taxonomy folder in instructlab-latest\n",
        "- **model_path = models/merlinite-7b-lab-Q4_K_M.gguf** - the default model is set to merlinite\n",
        "\n",
        "**Note:** The default directories for InstructLab are the following. If you initialize InstructLab on your own system, it will default to the following:\n",
        "* **Downloaded Models:**  ~/.cache/instructlab/models/ - Contains all downloaded large language models, including the saved output of ones you generate with ilab.\n",
        "* **Synthetic Data:** ~/.local/share/instructlab/datasets/ - Contains data output from the SDG phase, built on modifications to the taxonomy repository.\n",
        "* **Taxonomy:** ~/.local/share/instructlab/taxonomy/ - Contains the skill and knowledge data.\n",
        "* **Training Output:** ~/.local/share/instructlab/checkpoints/ - Contains the output of the training process.\n",
        "* **config.yaml:** ~/.config/instructlab/config.yaml - Contains the config.yaml file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ha4XTuHCvxge"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "if os.path.exists(\"sample_data\"):\n",
        "    print(\"removing sample_data\")\n",
        "    shutil.rmtree(\"sample_data\")\n",
        "os.chdir(\"ilab\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QX9s4XZx6rYF"
      },
      "outputs": [],
      "source": [
        "base_dir=\"/root/\"\n",
        "##Choose the base model as granite or mixtral\n",
        "model_dir=\"models\"\n",
        "model_name=\"granite-7b-lab-Q4_K_M.gguf\"\n",
        "model_path = os.path.join(model_dir, model_name)\n",
        "\n",
        "taxonomy_path='taxonomy'\n",
        "\n",
        "## Define the file name\n",
        "file_name = \"config.yaml\"\n",
        "if os.path.exists(file_name):\n",
        "    os.remove(file_name)\n",
        "    print(f\"ilab was already initialized. {file_name} has been deleted. Reinitialized\")\n",
        "else:\n",
        "    print(f\"ilab was not initialized yet. {file_name} does not exist.\")\n",
        "\n",
        "##Remove old data\n",
        "if os.path.exists(\"taxonomy\"):\n",
        "    print(\"removing taxonomy\")\n",
        "    shutil.rmtree(\"taxonomy\")\n",
        "if os.path.exists(base_dir+\".cache/instructlab\"):\n",
        "    print(\"removing \" + base_dir+\".cache/instructlab\")\n",
        "    shutil.rmtree(base_dir+\".cache/instructlab\")\n",
        "if os.path.exists(base_dir+\".config/instructlab\"):\n",
        "    print(\"removing \" + base_dir+\".config/instructlab\")\n",
        "    shutil.rmtree(base_dir+\".config/instructlab\")\n",
        "if os.path.exists(base_dir+\".local/share/instructlab\"):\n",
        "    print(\"removing \" + base_dir+\".local/share/instructlab\")\n",
        "    shutil.rmtree(base_dir+\".local/share/instructlab\")\n",
        "\n",
        "print(f\"ilab model is {model_path}.\")\n",
        "print('#############################################################')\n",
        "print(' ')\n",
        "\n",
        "command = f\"\"\"\n",
        "ilab config init<<EOF\n",
        "{taxonomy_path}\n",
        "Y\n",
        "{model_path}\n",
        "1\n",
        "9\n",
        "EOF\n",
        "\"\"\"\n",
        "\n",
        "## Using the ! operator to run the command\n",
        "!echo \"Running ilab config init\"\n",
        "!{command}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8rbeRKE6rYF"
      },
      "source": [
        "### 1.2.2 Display the config.yaml file\n",
        "We examine the base configuration for identifying parameters for changing in the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZflN-eeu6rYF",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "##to copy config.yaml to local directory\n",
        "!cp /root/.config/instructlab/config.yaml .\n",
        "!cat config.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAL743VA6rYH"
      },
      "source": [
        "### 1.2.3 Customize LLM Models and copy to notebook for use\n",
        "\n",
        "This cell changes the models to use for the generate stage. The mistral model as the teacher model in the generate step and as the student model to be trained.\n",
        "\n",
        "If you want to customize other models for generation or the training phase, you would specify the models in this step.\n",
        "\n",
        "This step specifies that the models to be used will be from this notebook's models directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QILAQZYY6rYH",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "##Use ruamel.yaml to load the yaml file to preserve comments\n",
        "import ruamel.yaml\n",
        "yaml = ruamel.yaml.YAML()\n",
        "with open('config.yaml', 'r') as file:\n",
        "    config = yaml.load(file)\n",
        "\n",
        "##Upate to use the same models and just change the directory\n",
        "teacher_model_path = \"models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n",
        "base_model_path = \"models/instructlab/granite-7b-lab\"\n",
        "##judge_model_path = \"models/prometheus-eval/prometheus-8x7b-v2.0\"\n",
        "\n",
        "##config['evaluate']['mt_bench']['judge_model'] = judge_model_path\n",
        "##config['evaluate']['mt_bench_branch']['judge_model'] = judge_model_path\n",
        "config['generate']['model'] = teacher_model_path\n",
        "config['generate']['teacher']['model_path']= teacher_model_path\n",
        "##config['train']['phased_mt_bench_judge']=judge_model_path\n",
        "\n",
        "## Save the updated config.yaml file\n",
        "yaml.default_flow_style=False\n",
        "with open('config.yaml', 'w') as file:\n",
        "    yaml.dump(config, file)\n",
        "\n",
        "##copy the config file to the .config/instructlab/ where it is used by InstructLab\n",
        "!cp config.yaml {base_dir}.config/instructlab/\n",
        "\n",
        "print(\"Updated config.yaml successfully.\\n\")\n",
        "!cat config.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dZgO-FZ6rYH"
      },
      "source": [
        "<a id=\"IL1_down\"></a>\n",
        "## Step 1.3 Download Models\n",
        "The models that will be used in the InstructLab processing are downloaded in this step. Additional steps can be added if other models are used in processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI-25zVd6rYH"
      },
      "source": [
        "### 1.3.1 Download the merlinite and mistral-7b-instruct-v0.2.Q4_K_M models\n",
        "\n",
        "The merlinite model will be used as the teacher model for the simple pipeline in the [Training with InstructLab](#IL2_0) notebook.\n",
        "\n",
        "The mistral-7b-instruct-v0.2.Q4_K_M model will be used as the teacher model for the full pipeline in the same notebook.\n",
        "\n",
        "The granite07b-lab.gguf model is a quantized version oft eh granite-7b-lab model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCa2va8r6rYH"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "hf_token=userdata.get('hf_token')\n",
        "models_dir=\"models\"\n",
        "!ilab model download --hf-token {hf_token} --model-dir {models_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d84cf2D6rYH",
        "tags": []
      },
      "source": [
        "### 1.3.2 Optionally Download the granite 7b safe tensors model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcYf96yxyr_X",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "##!ilab model download --repository instructlab/granite-7b-lab --hf-token {hf_token} --model-dir {models_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg-mhvIb6rYI",
        "tags": []
      },
      "source": [
        "### 1.3.3 Optionally download the prometheus-8x7b-v2.0 model\n",
        "The *prometheus-8x7b-v2.0* model is used as a judege model for multi-phase training and benchmark evaluation. This model is not required for simple or full training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hT6ZHGmx6rYI"
      },
      "outputs": [],
      "source": [
        "##!ilab model download --repository prometheus-eval/prometheus-8x7b-v2.0 --hf-token {hf_token} --model-dir {models_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDUmbbHLvxgg",
        "tags": []
      },
      "source": [
        "<a id=\"IL2_0\"></a>\n",
        "# Training with InstructLab\n",
        "\n",
        "\n",
        "This notebook demonstrates InstructLab, a model-agnostic open source AI project that facilitates contributions to Large Language Models (LLMs).\n",
        "\n",
        "This notebook is part of a sequential notebook set. Before using this notebook, please ensure that you have run the first notebook in this  section: [Configuring InstructLab](#IL1_0).\n",
        "\n",
        "In this notebook, we will demonstrate the following:\n",
        "- Querying the LLM before\n",
        "- Creating a question and answer data file\n",
        "- Generating synthetic data for training\n",
        "- Training the LLM with the generated data\n",
        "\n",
        "**Notes:** This notebook must be run with a GPU. If you are not running with a GPU, please select File->Hub Control Panel->Stop My Server, then Start My Server and the select GPU Session\n",
        "\n",
        "* [Step 1. Import Libraries and Check Configuration](#IL2_import)\n",
        "* [Step 2. Specify the Data for this Run](#IL2_data)\n",
        "* [Step 3. Create the Taxonomy Data Repository](#IL2_taxonomy)\n",
        "* [Step 4. Generate Synthetic Data](#IL2_generate)\n",
        "* [Step 5. Train Model](#IL2_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXtTRy6Ovxgg",
        "tags": []
      },
      "source": [
        "<a id=\"IL2_import\"></a>\n",
        "## Step 2.1 Import Libraries and Check Configuration\n",
        "\n",
        "### 2.1.1. Imports and configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIfYALELvxgg"
      },
      "outputs": [],
      "source": [
        "## standard imports\n",
        "import subprocess\n",
        "import json\n",
        "\n",
        "os.environ['NUMEXPR_MAX_THREADS'] = '64'\n",
        "il_data_path= '/home/jovyan/.local/share/instructlab/datasets/'\n",
        "with open('config.json', 'r') as f:\n",
        "    jsonData = json.load(f)\n",
        "with open('instructlab.json', 'r') as f:\n",
        "    jsonState = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezbxoKaAvxgg",
        "tags": []
      },
      "source": [
        "### 2.1.2. Optionally test the base model before adding data\n",
        "\n",
        "At this point you may wish to run an InstructLab server and run queries against the base model.\n",
        "\n",
        "This may be useful if you are working with new content and want to query the base model to ascertain the responses before InstructLab training.\n",
        "\n",
        "After training, the [Inferencing with InstructLab](./02_inferencing_with_InstructLab.ipynb) notebook allows you to ask questions to both the base and InstructLab trained models and compare answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXAjRQYzvxgg"
      },
      "outputs": [],
      "source": [
        "##!ilab model serve --model-path models/merlinite-7b-lab-Q4_K_M.gguf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnOLAXRxvxgh",
        "tags": []
      },
      "source": [
        "<a id=\"IL2_data\"></a>\n",
        "## Step 2.2 Specify the Data for this Run\n",
        "\n",
        "We've provided question-and-answer files for these datasets: \"2024 Oscar Awards Ceremony\" and \"Quantum Roadmap and Patterns\" and \"Artificial Intelligence Agents\". Feel free to choose one of these datasets, or select your own custom dataset in the cell below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltRTPBTVvxgh",
        "tags": []
      },
      "source": [
        "### 2.2.1 Optionally, Create your own data set for InstructLab\n",
        "\n",
        "Follow these steps to add your own dataset:\n",
        "1. Create your own **qna.yaml** file based on the example qna.yaml files provided in the /data/oscars, /data/quantum and /data/agentic_ai directories. Additional guidance on creating a properly formatted QNA.yaml file is found on the [InstructLab taxonomy readme](https://github.com/instructlab/taxonomy).\n",
        "1. Add your **qna.yaml** and sample **questions.txt** files to the **/data/your_content_1** folder or the **/data/your_content_2** folder.\n",
        "1. Right click on the **config.json** file and select Open With->Editort. Specify the **qna_location** where your data resides within the Dewy Decimal classification system. Close and save the **config.json** file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-uk0s0Uvxgh"
      },
      "outputs": [],
      "source": [
        "data_set = widgets.ToggleButtons(\n",
        "    options=['2024 Oscars', 'Quantum', 'Agentic AI', 'Your Content 1', 'Your Content 2'],\n",
        "    tooltips=['2024 Oscar Awards Ceremony', 'Quantum Roadmap and Patterns', 'Artificial Intelligence Agents', 'Your own uploaded content dataset 1', 'Your own uploaded content dataset 2'],\n",
        "    description='Dataset:', disabled=False, button_style='', style={\"button_width\": \"auto\"}\n",
        ")\n",
        "print(\"\\nSelect the QNA dataset to add:\")\n",
        "display(data_set)\n",
        "\n",
        "print(\"After choosing your dataset for this run, please select and run the following cell\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olkDUaKfvxgh"
      },
      "outputs": [],
      "source": [
        "print(\"Step 2.2 Choose the Dataset for this Run\")\n",
        "\n",
        "if data_set.value=='2024 Oscars':\n",
        "    use_case=\"oscars\"\n",
        "elif data_set.value=='Quantum':\n",
        "    use_case=\"quantum\"\n",
        "elif data_set.value=='Agentic AI':\n",
        "    use_case=\"agentic_ai\"\n",
        "elif data_set.value=='Your Content 1':\n",
        "    use_case=\"your_content_1\"\n",
        "elif data_set.value=='Your Content 2':\n",
        "    use_case=\"your_content_2\"\n",
        "else:\n",
        "    use_case=\"undefined\"\n",
        "\n",
        "if use_case==\"undefined\":\n",
        "    print(\"ERROR: Undefined data set: \" + data_set.value + \" data\")\n",
        "else:\n",
        "    jsonState[\"last_use_case\"]=data_set.value\n",
        "    with open('instructlab.json', 'w') as f:\n",
        "            json.dump(jsonState, f, indent=4)\n",
        "    qna_file=\"data/\" + use_case + \"/qna.yaml\"\n",
        "    qna_location=jsonData[\"use_cases\"][use_case][\"qna_location\"]\n",
        "\n",
        "    print(\"Using \" + data_set.value + \" data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQBSnoObvxgh"
      },
      "source": [
        "<a id=\"IL2_taxonomy\"></a>\n",
        "## Step 2.3 Create the Taxonomy Data Repository\n",
        "### 2.3.1 Delete the prior repository and clone the empty taxonomy repository\n",
        "We start with an empty repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2s_UhRCvxgh"
      },
      "outputs": [],
      "source": [
        "shell_command1 = f\"rm -rf taxonomy\"\n",
        "taxonomy_repo=jsonData[\"taxonomy_repo\"]\n",
        "shell_command2 = f\"git clone {taxonomy_repo}\"\n",
        "\n",
        "print(\"Step 2.3 Create the Taxonomy data Repository\")\n",
        "print(\"2.3.1 Delete the prior repository and clone the empty taxonomy repository\")\n",
        "!{shell_command1}\n",
        "!{shell_command2}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHE7DuEfvxgh",
        "tags": []
      },
      "source": [
        "### 2.3.2 View the beginning of the QNA file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAzNZ_d-vxgh"
      },
      "outputs": [],
      "source": [
        "def print_file_top():\n",
        "    print_lines=40\n",
        "    with open(qna_file, 'r') as input_file:\n",
        "        for line_number, line in enumerate(input_file):\n",
        "            if line_number > print_lines:  # line_number starts at 0.\n",
        "                break\n",
        "            print(line, end=\"\")\n",
        "\n",
        "print(\"2.3.2 Show QNA file\")\n",
        "print_file_top()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwHTXrIMvxgh",
        "tags": []
      },
      "source": [
        "### 2.3.3 Place the QNA file in the proper taxonomy directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQ7NxL1Nvxgi"
      },
      "outputs": [],
      "source": [
        "##Should produce !mkdir -p ./taxonomy/knowledge/textbooks/culture/movies/awards/oscars\n",
        "shell_command1 = f\"mkdir -p ./taxonomy/{qna_location}\"\n",
        "shell_command2 = f\"cp ./{qna_file} ./taxonomy/{qna_location}/qna.yaml\"\n",
        "\n",
        "print(\"2.3.3 Place the QNA file in the proper taxonomy directory\")\n",
        "print(\"Place QNA file in taxononmy as: /taxonomy/\"+qna_location+\"/qna.yaml\")\n",
        "!{shell_command1}\n",
        "!{shell_command2}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4sk413tvxgi"
      },
      "source": [
        "### 2.3.4 Verify the taxonomy\n",
        "We run the ilab taxonomy diff command to verify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmV6pHvuvxgi"
      },
      "outputs": [],
      "source": [
        "print(\"Verify the taxonomy\")\n",
        "!ilab taxonomy diff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFKW-EIVvxgi",
        "tags": []
      },
      "source": [
        "<a id=\"IL2_generate\"></a>\n",
        "## Step 2.4. Generate Synthetic Data\n",
        "\n",
        "This step will produce synthetic training data from the provided repository in the form of question and answer pairs. To generate synthetic data, InstructLab uses s large teacher model, such as Mixtral 8x7B, to create synthetic training data about the manually created data to train a small student model, such as the Merlinite 7B or Granite 7B models.\n",
        "\n",
        "You can skip this step and use the previously generated 500 synthetic samples found in the *generated* folder.\n",
        "\n",
        "### 2.4.1 Set data generation parameters\n",
        "\n",
        "#### Select pipeline\n",
        "\n",
        "InstructLab has three primary pipelines that can be used: simple, full and acellerated:\n",
        "- The **simple pipeline** runs fast and can be used for initial model and data testing.\n",
        "- The **full pipeline** runs all of the InstrctLab steps and takes more time but produces a better tuned model.\n",
        "- The **accelerated pipeline** runs the full pipeline processing using a GPU, so it produces a similarly tuned model in a shorter time.\n",
        "\n",
        "**Note:** If you are running with a new or modifed dataset, you may want to use the **Simple pipeline** for the first run to verify the configuration\n",
        "\n",
        "#### Sepect number of samples to generate\n",
        "\n",
        "Data generation takes 19 minutes for generating 15 synthetic data samples. You may wish to generate a small number on your first run to verify the QNA dataset format.\n",
        "\n",
        "To produce **sufficient synthetic data** to focus training on the new material, **about 30 synthetic questions and answer pairs need to be generated** for each question and answer pair provided. This will require a proportionally longer time to generate, but will provide better training.\n",
        "\n",
        "Before following these instructions, ensure the existing model you are adding skills or knowledge to is still running. Alternatively, ilab data generate can start a server for you if you provide a fully qualified model path via --model.\n",
        "\n",
        "To generate a synthetic dataset based on your newly added knowledge or skill set in taxonomy repository, run the following command:\n",
        "\n",
        "    ilab data generate\n",
        "\n",
        "#### **Simple Pipeline**\n",
        "\n",
        "The Simple Pipeline works solely with Merlinite 7b Lab as the teacher model. The Simple Pipeline is called without GPU acceleration as follows:\n",
        "\n",
        "    ilab data generate --pipeline simple\n",
        "\n",
        "#### **Full Pipeline**\n",
        "\n",
        "The Full Pipeline runs the full processing with a GPU. Currently, the Full Pipeline only supports the Mixtral and Mistral Instruct Family models as the teacher model.  This is due to only supporting specific model prompt templates.\n",
        "\n",
        "Using a non-default model such as Mixtral-8x7B-Instruct-v0.1) to generate data with the Full Pipeline:\n",
        "\n",
        "    ilab data generate --model ~/.cache/instructlab/models/mistralai/mixtral-8x7b-instruct-v0.1 --pipeline full --gpus 4\n",
        "\n",
        "**Note** Synthetic Data Generation can take from 15 minutes to 1+ hours to complete, depending on your computing resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rNYESyEvxgi",
        "tags": []
      },
      "outputs": [],
      "source": [
        "pipe2 = widgets.ToggleButtons(\n",
        "    options=['Simple', 'Full with GPU', 'Demo (Use prior data)'],\n",
        "    tooltips=['Ilab Simple Pipeline', 'Full Pipe running on a GPU', 'Demo Run with previously created data'],\n",
        "    description='Processing:', disabled=False, button_style='', style={\"button_width\": \"auto\"}\n",
        ")\n",
        "instr=widgets.ToggleButtons(\n",
        "    options=['15', '50', '200', '450 (default)', '1000'],\n",
        "    description='# of QNAs:',\n",
        "    disabled=False,\n",
        "    button_style='',\n",
        "    style={\"button_width\": \"auto\"}\n",
        ")\n",
        "\n",
        "print(\"Select Pipeline to use\")\n",
        "display(pipe2)\n",
        "display(instr)\n",
        "\n",
        "print(\"After making your selections for data generation, please select and run the following cell\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-so0dOw4vxgi",
        "tags": []
      },
      "source": [
        "### 2.4.2 Run data generation\n",
        "Data generation takes 19 minutes for generating 15 synthetic data samples and takes longer to generate more samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wn6gODEcvxgi",
        "tags": []
      },
      "outputs": [],
      "source": [
        "directory = \"data/\"+ use_case+\"/ilab_generated/\"\n",
        "if pipe2.value!='Demo (Use prior data)':\n",
        "    if instr.value == '15':\n",
        "        sdg_factor=\"--sdg-scale-factor 1\"\n",
        "    elif instr.value == '50':\n",
        "        sdg_factor=\"--sdg-scale-factor 3\"\n",
        "    elif instr.value == '200':\n",
        "        sdg_factor=\"--sdg-scale-factor 13\"\n",
        "    elif instr.value == '450 (default)':\n",
        "        sdg_factor=\"\"\n",
        "    else:\n",
        "        sdg_factor=\"--sdg-scale-factor 67\"\n",
        "    # 'Fast (Simple)', 'Full with CPU'\n",
        "    if pipe2.value == 'Simple':\n",
        "        pipeline = 'simple'\n",
        "        model = 'models/merlinite-7b-lab-Q4_K_M.gguf'\n",
        "        gpus = '--gpus 1'\n",
        "    elif pipe2.value == 'Full with GPU':\n",
        "        pipeline = 'full'\n",
        "        model = 'models/ibm-granite/granite-embedding-125m-english'\n",
        "        gpus = '--gpus 1'\n",
        "    else:\n",
        "        print(\"ERROR: Undefined pipeline\")\n",
        "\n",
        "    #Remove old data so there is only one test_merlinite and train_merlinite after generation\n",
        "    !rm -rf /home/jovyan/.local/share/instructlab/datasets/*\n",
        "    #shell_command = f\"ilab --verbose data generate --model {model} --num-cpus 10 {gpus} {sdg_factor} --taxonomy-path taxonomy --pipeline {pipeline} --max-num-tokens 512\"\n",
        "    shell_command = f\"ilab data generate --model {model} --num-cpus 10 {gpus} {sdg_factor} --taxonomy-path taxonomy --pipeline {pipeline} --max-num-tokens 512\"\n",
        "\n",
        "    print(\"Generating data\")\n",
        "    print(\"Running: !\"+shell_command)\n",
        "    !{shell_command}\n",
        "\n",
        "    #Rename results to  test_gen.jsonl and train_gen.jsonl and move to local data directory\n",
        "    if not os.path.exists(directory):\n",
        "        print(\"Create directory: \" + directory)\n",
        "        !mkdir {directory}\n",
        "    file_cnt=0\n",
        "    try:\n",
        "        for dirname in os.listdir(il_data_path):\n",
        "            date_path=il_data_path+'/'+ dirname + '/'\n",
        "            for filename in os.listdir(date_path):\n",
        "                if filename[:6]=='train_':\n",
        "                    train_name= 'train_gen.jsonl'\n",
        "                    print('Renaming '+ filename+ ' to ' + train_name)\n",
        "                    !mv {date_path+filename} {directory+train_name}\n",
        "                    file_cnt+=1\n",
        "                elif filename[:5]=='test_':\n",
        "                    test_name= 'test_gen.jsonl'\n",
        "                    print('Renaming '+ filename+ ' to ' + test_name)\n",
        "                    !mv {date_path+filename} {directory+test_name}\n",
        "                    file_cnt+=1\n",
        "        if file_cnt < 2:\n",
        "            print(\"ERROR: train_gen.jsonl and/or test.jsonl not created\")\n",
        "        elif os.path.getsize(directory+train_name) == 0:\n",
        "            print(\"ERROR: train_gen.jsonl file is empty\")\n",
        "        elif os.path.getsize(directory+test_name) == 0:\n",
        "            print(\"ERROR: test_gen.jsonl file is empty\")\n",
        "        else:\n",
        "            print(\"Training and test files successfully created in: \" + directory)\n",
        "    except:\n",
        "        print(\"Error running ilab generate, no synthetic data generated\")\n",
        "else:\n",
        "    print(\"Using previously generated data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0B_39Sqvxgj"
      },
      "source": [
        "### 2.4.3 Show examples of generated data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLqnVW7rvxgj"
      },
      "outputs": [],
      "source": [
        "print(\"2.4.3 Show examples of generated data\")\n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "    if filename[:9]=='train_gen':\n",
        "        with open(directory+filename, 'r') as syn_file:\n",
        "            cnt=0\n",
        "            for line_number, line in enumerate(syn_file):\n",
        "                if cnt >= 8:\n",
        "                    break\n",
        "                jsonLine= json.loads(line)\n",
        "                syn_user=jsonLine[\"user\"]\n",
        "                syn_assist=jsonLine[\"assistant\"]\n",
        "                #Remove \"Answer:\" and \"Response:\" from anawers for displaying\n",
        "                if syn_assist[:8]==\"Answer: \":\n",
        "                    syn_assist=syn_assist[8:]\n",
        "                cnt+=1\n",
        "                print(\"\\nQuestion: \"+syn_user+\"\\nAnswer: \"+syn_assist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvTYe8oHdm13",
        "tags": []
      },
      "source": [
        "<a id=\"IL2_train\"></a>\n",
        "## Step 2.5 Train the Model\n",
        "\n",
        "### 2.5.1 Select the model training pipeline\n",
        "\n",
        "InstructLab has three primary model training pipelines: simple, full (default), and accelerated. For all of the models, the training time can be limited by adjusting the num_epoch paramater. The maximum number of epochs for running the InstructLab end-to-end workflow is 10.\n",
        "\n",
        "#### **Simple pipeline**\n",
        "\n",
        "The simple pipeline uses an SFT Trainer on Linux and MLX on MacOS. This type of training takes roughly an hour and produces the lowest fidelity model but should indicate if your data is being picked up by the training process. The simple pipeline only works with Merlinite 7b Lab as the teacher model. For this Linux system, the trained model is saved in the models directory as ggml-model-f16.gguf.\n",
        "\n",
        "The command form is:\n",
        "\n",
        "    ilab model train --pipeline simple\n",
        "\n",
        "**Note:** This process will take a little while to complete (time can vary based on hardware and output of ilab data generate but on the order of 5 to 15 minutes)\n",
        "\n",
        "#### **Full pipeline**\n",
        "\n",
        "The full pipeline uses a custom training loop and data processing functions for the granite family of models. This loop is optimized for CPU and MPS functionality. Please use **--pipeline=full** in combination with **--device=cpu** for this Linus system. For a MacOS system you can use --device=mps (MacOS) or --device=cpu, however, MPS is optimized for better performance on MacOS systems. The full pipeline only works with Mixtral and Mistral Instruct Family models as the teacher model. For the full pipeline, the models are saved in the ~/.local/share/instructlab/checkpoints directory. The instructlab command \"ilab model evaluate\" can be used to choose the best one.\n",
        "\n",
        "The command form is:\n",
        "\n",
        "    ilab model train\n",
        "\n",
        "\n",
        "**Note:** This process will take a while to complete. If you run for ~8 epochs it will take several hours.\n",
        "\n",
        "#### **Accelerated pipeline**\n",
        "\n",
        "The accelerated uses the instructlab-training library which supports GPU accelerated and distributed training. The full loop and data processing functions are either pulled directly from or based off of the work in this library. For the accelerated pipeline, the models are saved in the ~/.local/share/instructlab/checkpoints directory. The instructlab command \"ilab model evaluate\" can be used to choose the best one. Training is support for GPU acceleration with Nvidia CUDA or AMD ROCm. Please see the GPU acceleration documentation for more details. At present, hardware acceleration requires a data center GPU or high-end consumer GPU with at least 18 GB free memory.\n",
        "\n",
        "The command form is:\n",
        "\n",
        "    ilab model train --pipeline accelerated --device cuda --data-path <path-to-sdg-data>\n",
        "\n",
        "#### **Multiphase**\n",
        "\n",
        "When running multi phase training evaluation is run on each phase, we will tell you which checkpoint in this folder performs the best. Train the model locally with multi-phase training and GPU acceleration. This results in the following workflow:\n",
        "1. We train the model on knowledge\n",
        "1. Evaluate the trained model to find the best checkpoint\n",
        "1. We train the model on skills\n",
        "1. We evaluate the model to find the best overall checkpoint\n",
        "\n",
        "Phase 1 models saved in ~/.local/share/instructlab/phased/phase1/checkpoints (Knowledge training). Phase 2 models saved in ~/.local/share/instructlab/phased/phase2/checkpoints (Skills training). Evaluation is run for phase 2 to identify the best checkpoint.\n",
        "\n",
        "A multiphase training is of the form:\n",
        "\n",
        "    ilab model train --strategy lab-multiphase --phased-phase1-data <knowledge train messages jsonl> --phased-phase2-data <skills train messages jsonl> -y\n",
        "\n",
        "**Note:** This command may take 3 or more hours depending on the size of the data and number of training epochs you run.\n",
        "\n",
        "#### **Skills only**\n",
        "\n",
        "Phase 2 models saved in ~/.local/share/instructlab/phased/phase2/checkpoints (Skills training). Evaluation is run for phase 2 to identify the best checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UrXh9-nvxgj"
      },
      "outputs": [],
      "source": [
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "pipe3 = widgets.ToggleButtons(\n",
        "    options=['Simple', 'Full with CPU', 'Accelerated GPU', 'Demo (Use prior data)'],\n",
        "    tooltips=['Ilab Simple Pipeline', 'Full Pipe running only on a CPU', 'Accelerated pipeline run on a GPU', 'Demo Run with previously created data'],\n",
        "    description='Processing', disabled=False, button_style='', style={\"button_width\": \"auto\"}\n",
        ")\n",
        "epoch=widgets.ToggleButtons(\n",
        "    options=['1', '2', '3', '4', '5', '10', '15'],\n",
        "    description='Epochs:',\n",
        "    disabled=False, button_style='', style={\"button_width\": \"auto\"}\n",
        ")\n",
        "it=widgets.ToggleButtons(\n",
        "    options=['1', '3', '5','10','20','50','100','200'],\n",
        "    description='Iterations:',\n",
        "    disabled=False, button_style='', style={\"button_width\": \"auto\"}\n",
        ")\n",
        "\n",
        "print(\"Select to Continue or to Train the model\")\n",
        "##pipe3.value=pipe.value\n",
        "display(pipe3)\n",
        "display(epoch)\n",
        "display(it)\n",
        "\n",
        "print(\"After choosing your training options, please select and run the following cell\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUPA7SpTvxgj"
      },
      "source": [
        "### 2.5.2 Run the model training\n",
        "\n",
        "Model training takes 10 minutes for 1 epoch and 1 iteration. This minimal training could be used for testing the generation and training for a new set of data.\n",
        "\n",
        "To produce a higher quality model, more epochs and iterations are needed for refining the model. This will require a proportionally longer time to train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcIjCQUjvxgj",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "data_path=\"data/\"+ use_case+\"/ilab_generated/\"\n",
        "train_data=data_path+\"train_gen.jsonl\"\n",
        "model_path=\"models/instructlab/granite-7b-lab\"\n",
        "##model_path='/home/jovyan/.cache/instructlab/models/instructlab/granite-7b-lab'\n",
        "trained_model_path=\"data/\"+ use_case+\"/new_model/\"\n",
        "\n",
        "##'Simple (Fast)', 'Full with CPU', 'Accelerated GPU','DDemo (Use prior data)'\n",
        "if pipe3.value=='Demo (Use prior data)':\n",
        "    print(\"Using previously trained data\")\n",
        "else:\n",
        "    file_cnt=0\n",
        "    for filename in os.listdir(data_path):\n",
        "        if filename[:15]=='train_gen.jsonl': file_cnt+=1\n",
        "        elif filename[:14]=='test_gen.jsonl': file_cnt+=1\n",
        "    if file_cnt < 2 or os.path.getsize(directory+train_name) < 5 or os.path.getsize(directory+test_name) < 5:\n",
        "        print(\"ERROR: train_gen.jsonl and/or test.jsonl are not present or too small\")\n",
        "\n",
        "    if not os.path.exists(trained_model_path):\n",
        "        print(\"Create directory: \" + trained_model_path)\n",
        "        !mkdir {trained_model_path}\n",
        "    ep=int(epoch.value)\n",
        "    its=int(it.value)\n",
        "    if pipe3.value=='Simple':\n",
        "        print(\"Train with simple pipeline with a GPU\")\n",
        "        shell_command = f\"ilab model train --pipeline simple --model-path {model_path} --data-path {data_path} --device cpu --num-epochs {ep} --iters {its}\"\n",
        "    if pipe3.value=='Full with CPU':\n",
        "        print(\"Train with a CPU\")\n",
        "        shell_command = f\"ilab model train --pipeline full --model-path {model_path} --data-path {train_data} --device cpu\"\n",
        "    elif pipe3.value=='Accelerated GPU':\n",
        "        print(\"Train with a GPU\")\n",
        "        shell_command = f\"ilab model train --pipeline accelerated --device cuda --model-path {model_path} --data-path {train_data} --num-epochs {ep} --iters {its}\"\n",
        "\n",
        "    print(\"Running: !\"+shell_command)\n",
        "    !{shell_command}\n",
        "    if pipe3.value=='Accelerated GPU' or pipe3.value=='Accelerated GPU with 4b Quantization':\n",
        "        !ilab model evaluate --benchmark mmlu\n",
        "    #Move the model to the use_case/new_model directory\n",
        "    print(\"Moving the trained model to the directory: \"+trained_model_path)\n",
        "    !mv /home/jovyan/.local/share/instructlab/checkpoints/ggml-model-f16.gguf {trained_model_path}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7eosdzEvxgj"
      },
      "source": [
        "<a id=\"IL2_test\"></a>\n",
        "## Step 6 Test the Model\n",
        "### 2.6.1 Run test on the model to see how it performs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJt89CzPvxgk"
      },
      "outputs": [],
      "source": [
        "##!ilab model test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RijXmAWUvxgk"
      },
      "source": [
        "### 2.6.2 Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zl_m3OGUvxgk"
      },
      "outputs": [],
      "source": [
        "##ILAB_MODELS_DIR=$HOME/.local/share/instructlab/models\n",
        "##shell_command=f\"ilab model evaluate --benchmark mmlu --model {ILAB_MODELS_DIR}/instructlab/granite-7b-test\"\n",
        "##!{shell_command}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SogwbM1vxgk",
        "tags": []
      },
      "source": [
        "<a id=\"I3_0\"></a>\n",
        "# 3. Inferencing with InstructLab\n",
        "\n",
        "You have now completed InstructLab training. You can now select to run the [Inferencing with InstructLab](./02_inferencing_with_InstructLab.ipynb) notebook to ask questions to both the base and InstructLab trained models and to compare answers.\n",
        "\n",
        "This notebook is part of a sequential notebook set. Before using this notebook, please ensure that you have reviewed the first and second notebooks in the set:\n",
        "* <a href=\"#IL1_0\">Configuring InstructLab</a>\n",
        "* <a href=\"#IL2_0\">Training with InstructLab</a>\n",
        "\n",
        "The second notebook within this set showcases the generation of synthetic data utilizing InstructLab. It subsequently demonstrates how a large language model (LLM) can be effectively trained on this synthetic dataset. In current notebook, Both the pre-trained LLM and the LLM trained on the generated synthetic data are evaluated against a predefined set of questions to assess their respective performance.\n",
        "\n",
        "Inferences steps are:\n",
        "\n",
        "* <a href=\"#IL3_1\">Step 1. Import Libraries </a>\n",
        "* <a href=\"#IL3_2\">Step 2. Load the Base Model and the InstructLab Trained Model</a>\n",
        "* <a href=\"#IL3_3\">Step 3. Define a Function to Perform Inference on Base and Trained Models </a>\n",
        "* <a href=\"#IL3_4\">Step 4. Run Interactive Q&A Session with Base and Trained Models to Evaluate Performance</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFcgMkTZvxgk"
      },
      "source": [
        "<a id=\"IL3_1\"></a>\n",
        "## Step 3.1 Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NernbGwFvxgk"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "with open('instructlab.json', 'r') as f:\n",
        "    jsonState = json.load(f)\n",
        "\n",
        "print(f\"Imports completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eZQ4_zXvxgk"
      },
      "source": [
        "<a id=\"IL3_2\"></a>\n",
        "## Step 3.2 Load the Base Model and the InstructLab Trained Model\n",
        "The data set that was used in that last run of the [Training with InstructLab](#IL2_0) notebook is preselected for inferencing. You may select an alternative dataset if you wish.\n",
        "You can select the trained model to use for inferencing from:\n",
        "* **Fine Tuned Model** - A previously trained fine tuned model to demonstrate inferencing.\n",
        "* **Newly Trained Model** - Your trained model from prior runs of the [Training with InstructLab](#IL2_0) notebook. You can optionally place a model you wish to make comparisons with in the */data/data_set/new_model* directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCe2C_bGvxgk"
      },
      "outputs": [],
      "source": [
        "data_set = widgets.ToggleButtons(\n",
        "    options=['2024 Oscars', 'Quantum', 'Agentic AI', 'Your Content 1', 'Your Content 2'],\n",
        "    tooltips=['2024 Oscar Awards Ceremony', 'Quantum Roadmap and Patterns', 'Artificial Intelligence Agents', 'Your own uploaded content dataset 1', 'Your own uploaded content dataset 2'],\n",
        "    description='Dataset:', disabled=False, button_style='', style={\"button_width\": \"auto\"}\n",
        ")\n",
        "model = widgets.ToggleButtons(\n",
        "    options=['Fine Tuned Model', 'Newly Tuned Model'],\n",
        "    tooltips=['Tested fine tuned model', 'Newly tuned model'],\n",
        "    description='Model:', disabled=False, button_style='', style={\"button_width\": \"auto\"}\n",
        ")\n",
        "\n",
        "print(\"\\nPlease select correct document which was used in notebook 01_training_with_InstructLab\")\n",
        "print(\"\\nSelect the content (Last used in training is preselected):\")\n",
        "data_set.value=jsonState[\"last_use_case\"]\n",
        "display(data_set)\n",
        "display(model)\n",
        "\n",
        "print(\"After choosing your dataset for inferencing, select the following cell and continue running the notebook\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5l5lIshovxgk"
      },
      "outputs": [],
      "source": [
        "print(\"Using Data Set: \" + data_set.value)\n",
        "if data_set.value=='2024 Oscars':\n",
        "    use_case=\"oscars\"\n",
        "elif data_set.value=='Quantum':\n",
        "    use_case=\"quantum\"\n",
        "elif data_set.value=='Agentic AI':\n",
        "    use_case=\"agentic_ai\"\n",
        "elif data_set.value=='Your Content 1':\n",
        "    use_case=\"your_content_1\"\n",
        "elif data_set.value=='Your Content 2':\n",
        "    use_case=\"your_content_2\"\n",
        "else:\n",
        "    print(\"ERROR: Please select correct document which was used in notebook 01_training_with_InstructLab\")\n",
        "\n",
        "if model.value == 'Fine Tuned Model':\n",
        "    directory=\"/fine_tuned_models/\"+use_case+\"/\"\n",
        "else:\n",
        "    directory=\"/data/\"+ use_case+\"/new_model/\"\n",
        "\n",
        "notebook_dir=os.getcwd()\n",
        "os.chdir('/home/jovyan/')\n",
        "pwd= os.getcwd()\n",
        "\n",
        "base_model_path = notebook_dir +\"/models/granite-7b-lab-Q4_K_M.gguf\"\n",
        "trained_model_path = notebook_dir + directory+ \"ggml-model-f16.gguf\"\n",
        "\n",
        "print(\"Base model directory: \"+base_model_path)\n",
        "print(\"Trained model directory: \"+trained_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3fc8vdqvxgl",
        "tags": []
      },
      "source": [
        "<a id=\"IL3_3\"></a>\n",
        "## Step 3.3 Define a Function to Perform Inference on Base and Trained Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlI5y7Ngvxgl"
      },
      "outputs": [],
      "source": [
        "def model_inference(base_model_path, trained_model_path):\n",
        "    _DEFAULT_TEMPLATE = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
        "\n",
        "    Current conversation:\n",
        "    Human: {input}\n",
        "    AI:\"\"\"\n",
        "\n",
        "    base_llm = LlamaCpp(model_path=base_model_path,\n",
        "                   verbose=False,\n",
        "                   n_gpu_layers=25,\n",
        "                   max_tokens=90,\n",
        "                   temperature=0,\n",
        "                   top_k=1\n",
        "                  )\n",
        "    trained_llm = LlamaCpp(model_path=trained_model_path,\n",
        "                   verbose=False,\n",
        "                   n_gpu_layers=25,\n",
        "                   max_tokens=90,\n",
        "                   temperature=0,\n",
        "                   top_k=1\n",
        "                  )\n",
        "\n",
        "    PROMPT = PromptTemplate( input_variables=[\"input\"],\n",
        "                            template=_DEFAULT_TEMPLATE\n",
        "                            )\n",
        "\n",
        "    chain1 = PROMPT | base_llm | StrOutputParser()\n",
        "    chain2 = PROMPT | trained_llm | StrOutputParser()\n",
        "\n",
        "    while True:\n",
        "        question = input(\"Ask me a question (type 'exit' to end): \")\n",
        "        if question.lower() == 'exit':\n",
        "            print(\"Exiting this Q&A session.\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"You asked: \", question)\n",
        "            answer1 = chain1.invoke(question)\n",
        "            answer1= answer1.split('Human',1)[0]\n",
        "            print (\"Base Model Answer: \",answer1)\n",
        "            answer2 = chain2.invoke(question)\n",
        "            answer2= answer2.split('Human',1)[0]\n",
        "            print (\"Trained Model Answer: \",answer2)\n",
        "\n",
        "print(f\"Function to perform inference on LLMs defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWNj4u9Vvxgl",
        "tags": []
      },
      "source": [
        "<a id=\"IL3_4\"></a>\n",
        "## Step 3.4 Run Interactive Q&A Session with Base and Trained Models to Evaluate Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZiWmDChvxgl",
        "tags": []
      },
      "source": [
        "### 3.4.1 Sample questions that can be asked to LLM\n",
        "\n",
        "The following are sample questions derived from the data used to generate synthetic data, which was then employed to train the language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVp-BeByvxgl"
      },
      "outputs": [],
      "source": [
        "##Display Sample Questions\n",
        "with open(notebook_dir+'/data/' + use_case + '/questions.txt') as f:\n",
        "    for line in f.readlines():\n",
        "        display(widgets.HTML(Norm+line))\n",
        "\n",
        "print(\"Processing may take several minutes on the first run...\")\n",
        "model_inference(base_model_path, trained_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K4lhpzTvxgl",
        "tags": []
      },
      "source": [
        "<a id=\"IL3_conclusion\"></a>\n",
        "# Conclusion\n",
        "\n",
        "This notebook demonstrated utilizing InstructLab for introducing datasets, data generation, model training, and model creation. This notebook produced an InstructLab trained model that was used for inferencing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63hBXXa0vxgl"
      },
      "source": [
        "<a id=\"IL3_learn\"></a>\n",
        "# Learn More\n",
        "\n",
        "Proceed to run the [Inferencing with InstructLab](./02_inferencing_with_InstructLab.ipynb) notebook run inferencing on the InstructLab trained model. This will allow you to interact with your model to see how well it performs on queries, both before it was trained and after InstrutLab training\n",
        "\n",
        "This notebook is based on the InstructLab CLI repository available [here](https://github.com/instructlab/instructlab).\n",
        "\n",
        "InstructLab uses a novel synthetic data-based alignment tuning method for Large Language Models introduced in this [paper](https://arxiv.org/abs/2403.01081).\n",
        "\n",
        "Contact us by email to ask questions, discuss potential use cases, or schedule a technical deep dive. The contact email is IBM.Research.JupyterLab@ibm.com."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poB7nDmcvxgl"
      },
      "source": [
        "© 2025 IBM Corporation"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}